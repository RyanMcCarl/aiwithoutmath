# Rational agent

In AI, an _agent_ is something that acts (such as a software “bot” or a robot). A _rational agent_ is an agent that tries to achieve the best outcome. Agents are programmed to view some outcomes as better than others. The “best outcome,” as understood by the agent, is sometimes called the agent’s _objective function_.

## How to talk about rational agents

The idea that the agent tries to achieve the best outcome is sometimes phrased as: the agent tries to _maximize utility_ or _maximize expected utility_. “Utility” is a synonym for things the agent values. It is sometimes equated with “happiness.”

Thus, the following sentences are approximately equivalent: “an agent attempts to maximize its objective function”; “an agent does what it expects will makes the agent happiest”; “an agent does what it expects will maximize its utility”; “an agent tries to achieve the best expected outcome.”

Some philosophers argue that human beings try (or should try) to act in a way that maximizes their own utility, or that laws and social rules should be designed to maximize overall utility. This position is called _utilitarianism_. The discipline of economics is largely based on the assumption that people are utility-maximizers.